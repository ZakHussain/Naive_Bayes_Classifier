{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES CLASSIFIER\n",
    "**Author: Zak Hussain**  \n",
    "**Data: 2019/09/31**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math \n",
    "\n",
    "class HamSpamClassifier():  \n",
    "    \n",
    "    def __init__(self:object): \n",
    "        # constansts \n",
    "        self._alpha = 1.0 \n",
    "        self._V = 200000   \n",
    "        \n",
    "        # model: ham table, spam table \n",
    "        self.ham_model = {}\n",
    "        self.total_ham_words = 0 \n",
    "        self.logProb_ham_email = 0 \n",
    "\n",
    "        self.spam_model = {}\n",
    "        self.total_spam_words = 0\n",
    "        self.logProb_spam_email = 0 \n",
    "        \n",
    "    def fit(self:object, HAM_train:str, SPAM_train:str): \n",
    "        \"\"\"Creates the model from the files' training directories.\"\"\"\n",
    "        \n",
    "        # count the word-type frequencies. \n",
    "        self.ham_model = self.__count_freq(HAM_train) \n",
    "        self.spam_model = self.__count_freq(SPAM_train) \n",
    "        \n",
    "        # compute the total words in ham, then the totala words in spam \n",
    "        self.total_ham_words =  sum([self.ham_model[key]['freq'] for key in self.ham_model.keys()])\n",
    "        self.total_spam_words = sum([self.spam_model[key]['freq'] for key in self.spam_model.keys()])\n",
    "            \n",
    "        # compute the probability of seeing the word in the given class: P(w_i | C). \n",
    "        self.__compute_prob(self.ham_model, self.total_ham_words)\n",
    "        self.__compute_prob(self.spam_model, self.total_spam_words)\n",
    "              \n",
    "        # Use the Laplace Smooth to bring pull our distribution towards a uniform state.\n",
    "        self.__laplace(self.ham_model, self.total_ham_words)\n",
    "        self.__laplace(self.spam_model, self.total_spam_words)  \n",
    "        \n",
    "        # map the P(w_i | C) from the multiplicative domain to the additive. \n",
    "        self.__compute_log_prob(self.ham_model) \n",
    "        self.__compute_log_prob(self.spam_model) \n",
    "        \n",
    "        # compute the probability of a class, count(class)/count(P_ham and P_spam)\n",
    "        self.__compute_log_P_classes(HAM_train, SPAM_train)\n",
    "        \n",
    "    def predict(self:object, X_test_dir:str) -> dict: \n",
    "        \"\"\"given the test_set's directory path, classify the emails as Spam or Ham\"\"\"\n",
    "        predictions = {}  \n",
    "        \n",
    "        # sum the logprob of a word_i given its class: logprob(w_i | c), for all words in test set\n",
    "        for filename in os.listdir(X_test_dir): \n",
    "            email = open(X_test_dir + filename, 'r')\n",
    "            LogProb_ham_sum = 0\n",
    "            LogProb_spam_sum = 0\n",
    "            for line in email: \n",
    "                word = line[:-1]\n",
    "                \n",
    "                # if I have seen the word before: \n",
    "                if word in self.ham_model.keys() and word in self.spam_model.keys(): \n",
    "                    LogProb_ham_sum += self.ham_model[word]['log_prob']\n",
    "                    LogProb_spam_sum += self.spam_model[word]['log_prob']\n",
    "                elif word in self.ham_model.keys():\n",
    "                    # I have seen the word in ham, but not in spam \n",
    "                    LogProb_ham_sum += self.ham_model[word]['log_prob']\n",
    "                    LogProb_spam_sum += math.log(self._alpha / (self.total_spam_words + (self._V*self._alpha))) \n",
    "                elif word in self.spam_model.keys():\n",
    "                    # I have seen the word in spam, but not in ham \n",
    "                    LogProb_spam_sum += self.spam_model[word]['log_prob']\n",
    "                    LogProb_ham_sum += math.log(self._alpha / (self.total_ham_words + (self._V*self._alpha))) \n",
    "                else: \n",
    "                    # I have not seen the word in either. \n",
    "                    LogProb_ham_sum += math.log(self._alpha / (self.total_ham_words + (self._V*self._alpha)))\n",
    "                    LogProb_spam_sum += math.log(self._alpha / (self.total_spam_words + (self._V*self._alpha))) \n",
    "               \n",
    "            email.close()\n",
    "            \n",
    "            # c_i = logprob(c) +  E(logprob(w_i | c)\n",
    "            ham_pred = self.logProb_ham_email + LogProb_ham_sum \n",
    "            spam_pred = self.logProb_spam_email + LogProb_spam_sum \n",
    "        \n",
    "            # prediction = argmax(c_1, c_2)\n",
    "            if ham_pred >= spam_pred: \n",
    "                predictions[filename] = (ham_pred, spam_pred, 'ham')\n",
    "            else: \n",
    "                predictions[filename] = (ham_pred, spam_pred, 'spam')\n",
    "                \n",
    "        return predictions \n",
    "        \n",
    "    def evaluate_model(self:object, predictions:list, y_test:list):\n",
    "        \"\"\"evaluate the accuracy of the model given the predictions, and actual labels.\"\"\"\n",
    "        pass \n",
    "    \n",
    "    def tune_Laplace_params(self:object, a:float, newV:int = 200000): \n",
    "        \"\"\"Update alpha and Vfor Laplace Smoothing. the model is updated automatically.\"\"\"\n",
    "        self._alpha = a\n",
    "        self._V = newV\n",
    "        \n",
    "        # automatically update the model on the new paramaters \n",
    "        self.__laplace(self.ham_model, self.total_ham_words)\n",
    "        self.__laplace(self.spam_model, self.total_spam_words)\n",
    "        \n",
    "        # update the new logProb for both the ham and spam models\n",
    "        self.__compute_log_prob(self.ham_model)\n",
    "        self.__compute_log_prob(self.spam_model)\n",
    "        \n",
    "    def get_models(self:object): \n",
    "        \"\"\"Returns the ham and spam tables.\"\"\"\n",
    "        return self.ham_model, self.spam_model\n",
    "                       \n",
    "    def __count_freq(self:object, folder_path:str) -> dict: \n",
    "        \"\"\"counts instances of words found in folder.\n",
    "\n",
    "            Args: \n",
    "                folder_path (str): folder path containing corpus.\n",
    "\n",
    "            Return: \n",
    "                dict containing (word : word_count) KV pairs. \n",
    "        \"\"\"\n",
    "        d = {} # stores the frequency of a word \n",
    "        for filename in os.listdir(folder_path): \n",
    "            file = open(folder_path + filename, 'r')\n",
    "            for line in file:\n",
    "                key = line[:-1]\n",
    "\n",
    "                if key in d: \n",
    "                    d[key]['freq'] += 1\n",
    "                else:\n",
    "                    d[key] = {'freq':1}\n",
    "            file.close() \n",
    "        return d\n",
    "    \n",
    "    def __compute_prob(self:object, words:dict, total_words:int): \n",
    "        \"\"\"updates the dictionary value associated with a word to include prob of the word occuring.\n",
    "        \n",
    "            Args: \n",
    "                words (dict): dictionary containing word:dict KV pairs, \n",
    "                where the word is a type gathered from the training data,\n",
    "                and associated value dict contains contains characteristics about \n",
    "                that word. \n",
    "                \n",
    "                total_words (int): the total number of words found within a class's \n",
    "                training set. \n",
    "        \"\"\"\n",
    "        for key in words: \n",
    "            words[key]['prob_word'] = words[key]['freq'] / total_words\n",
    "\n",
    "    def __laplace(self:object, words:dict, total_words:int):   \n",
    "        \"\"\"Apply Laplace Smoothing to distribute frequencies\"\"\"\n",
    "        for key in words: \n",
    "            words[key]['laplace'] = (words[key]['freq'] + self._alpha) / (total_words + (self._V * self._alpha)) # normalize, given unseen words\n",
    "            \n",
    "    def __compute_log_prob(self:object, words:dict): \n",
    "        \"\"\"uses the laplace values, in the dictionary-value of each word, an finds the log_prob of the word occuring. \n",
    "        \"\"\"\n",
    "        for key in words: \n",
    "            words[key]['log_prob'] = math.log(words[key]['laplace'])    \n",
    "  \n",
    "    def __compute_log_P_classes(self:object, HAM_path:str, SPAM_path:str): \n",
    "        \"\"\"Computes the log(P(Ham)) and log(P(Spam)), and updates P_ham_emails and P_spam_emails\n",
    "        \n",
    "            Args: \n",
    "                HAM_path (str): the filepath to the directory containing HAM emails. \n",
    "                SPAM_path (str): the filepath to the directory containing SPam emails. \n",
    "        \"\"\"\n",
    "        # for each directory, tally the number of files \n",
    "        HAM_file_tally = sum([1 for file in os.listdir(HAM_path)])      \n",
    "        SPAM_file_tally = sum([1 for file in os.listdir(SPAM_path)])\n",
    "        total_tally = HAM_file_tally + SPAM_file_tally\n",
    "\n",
    "        # compute the probability of the classes \n",
    "        self.logProb_ham_email = math.log(HAM_file_tally / total_tally)\n",
    "        self.logProb_spam_email = math.log(SPAM_file_tally / total_tally) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq': 60,\n",
       " 'prob_word': 0.0007300691132093838,\n",
       " 'laplace': 0.0002161710089870439,\n",
       " 'log_prob': -8.439440755242437}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_path = '../Data/data/ham/'\n",
    "spam_path = '../Data/data/spam/'\n",
    "test_path = '../Data/data/test/'\n",
    "\n",
    "HS_clf = HamSpamClassifier() \n",
    "HS_clf.fit(ham_path, spam_path)\n",
    "\n",
    "ham_model, spam_model = HS_clf.get_models()\n",
    "\n",
    "ham_model['At']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq': 1,\n",
       " 'prob_word': 1.2644143232854542e-05,\n",
       " 'laplace': 7.166198475032965e-06,\n",
       " 'log_prob': -11.84613524269797}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_model['At']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.words': (-14163.06312692431, -13406.153539262092, 'spam'),\n",
       " '10.words': (-2893.846357314009, -2431.070884969585, 'spam'),\n",
       " '100.words': (-144.25109229726633, -144.843518949235, 'ham'),\n",
       " '11.words': (-1732.0446505019277, -1869.855967297329, 'ham'),\n",
       " '12.words': (-1011.2871210617694, -1089.831069063449, 'ham'),\n",
       " '13.words': (-1884.450426450807, -1674.5782742060924, 'spam'),\n",
       " '14.words': (-828.3443877993997, -793.3529100259112, 'spam'),\n",
       " '15.words': (-88.54534951647017, -82.80516366322941, 'spam'),\n",
       " '16.words': (-6365.249062147799, -6403.513175139366, 'ham'),\n",
       " '17.words': (-353.5568665161027, -340.7739697723117, 'spam'),\n",
       " '18.words': (-995.1632353139026, -870.1977477472184, 'spam'),\n",
       " '19.words': (-3955.387807962444, -3542.5238305386806, 'spam'),\n",
       " '2.words': (-883.0994758574856, -941.6617586498218, 'ham'),\n",
       " '20.words': (-2405.4062853667547, -2029.1792808699968, 'spam'),\n",
       " '21.words': (-772.0053100190362, -712.7061191993727, 'spam'),\n",
       " '22.words': (-604.2741977289121, -713.0585612706298, 'ham'),\n",
       " '23.words': (-1334.006435803979, -1301.093610389905, 'spam'),\n",
       " '24.words': (-2405.9891462082123, -2390.992548106394, 'spam'),\n",
       " '25.words': (-477.6051027183586, -476.4927320838012, 'spam'),\n",
       " '26.words': (-1228.7718321449752, -1304.589332192648, 'ham'),\n",
       " '27.words': (-1366.7948841660477, -1541.3363409919687, 'ham'),\n",
       " '28.words': (-4161.377823842975, -3469.3489646559387, 'spam'),\n",
       " '29.words': (-596.9236078391478, -668.7453895925532, 'ham'),\n",
       " '3.words': (-458.523355626839, -516.5693276847155, 'ham'),\n",
       " '30.words': (-1683.5476666917411, -1822.6053933603644, 'ham'),\n",
       " '31.words': (-1221.6011693253224, -1122.8161787790084, 'spam'),\n",
       " '32.words': (-1119.2279394300097, -1107.0780078955206, 'spam'),\n",
       " '33.words': (-1108.5921354312984, -1197.5598957904074, 'ham'),\n",
       " '34.words': (-2817.1731478683187, -2925.502346218105, 'ham'),\n",
       " '35.words': (-754.4309639617827, -690.238616751103, 'spam'),\n",
       " '36.words': (-2240.5711856013527, -1800.735992176026, 'spam'),\n",
       " '37.words': (-176.39755185238042, -160.57484699236545, 'spam'),\n",
       " '38.words': (-222.8508501351161, -223.50242718597258, 'ham'),\n",
       " '39.words': (-442.81747889789096, -501.6526737267985, 'ham'),\n",
       " '4.words': (-653.2705888901953, -674.1994317417722, 'ham'),\n",
       " '40.words': (-2251.3240732498793, -2184.1413617700814, 'spam'),\n",
       " '41.words': (-678.1078917389212, -706.6379054125659, 'ham'),\n",
       " '42.words': (-1005.5414102009645, -914.4096663773373, 'spam'),\n",
       " '43.words': (-213.35534853006774, -194.3214932512313, 'spam'),\n",
       " '44.words': (-159.58949982545784, -143.68158534749776, 'spam'),\n",
       " '45.words': (-3148.97470749889, -2642.60368839275, 'spam'),\n",
       " '46.words': (-2559.47854538981, -2224.3600668078643, 'spam'),\n",
       " '47.words': (-567.0810975940813, -649.6474080641963, 'ham'),\n",
       " '48.words': (-3774.561787549902, -3905.3658420059787, 'ham'),\n",
       " '49.words': (-7979.811228168041, -8154.286321491897, 'ham'),\n",
       " '5.words': (-151.2969226135489, -151.16453625965494, 'spam'),\n",
       " '50.words': (-2893.846357314009, -2431.070884969585, 'spam'),\n",
       " '51.words': (-223.28665296827603, -222.56054090690952, 'spam'),\n",
       " '52.words': (-634.6237680818081, -708.1752911175455, 'ham'),\n",
       " '53.words': (-1077.6002595251564, -1218.6860377358685, 'ham'),\n",
       " '54.words': (-925.8696225259092, -1007.7348626041884, 'ham'),\n",
       " '55.words': (-9101.519411719628, -7639.347224352461, 'spam'),\n",
       " '56.words': (-2297.805425931995, -2409.778187201305, 'ham'),\n",
       " '57.words': (-533.1512843943599, -588.1010533935261, 'ham'),\n",
       " '58.words': (-632.7774328381478, -614.3574979616528, 'spam'),\n",
       " '59.words': (-2364.060900423481, -2587.2498765569553, 'ham'),\n",
       " '6.words': (-947.7931776902592, -1037.2337830600231, 'ham'),\n",
       " '60.words': (-1388.3821463505537, -1577.3958274067147, 'ham'),\n",
       " '61.words': (-1043.1281593888486, -1126.81875373275, 'ham'),\n",
       " '62.words': (-796.0668227633577, -876.7421363281588, 'ham'),\n",
       " '63.words': (-959.0871006038597, -881.1306262056892, 'spam'),\n",
       " '64.words': (-1717.9354205546842, -1887.249943495758, 'ham'),\n",
       " '65.words': (-998.8669160184786, -870.4213472677484, 'spam'),\n",
       " '66.words': (-330.12465636208793, -331.271475195983, 'ham'),\n",
       " '67.words': (-364.79140160311124, -407.5054900973553, 'ham'),\n",
       " '68.words': (-772.0053100190362, -712.7061191993727, 'spam'),\n",
       " '69.words': (-4151.442469001595, -3463.3084140615247, 'spam'),\n",
       " '7.words': (-1314.8750904963995, -1523.783428230152, 'ham'),\n",
       " '70.words': (-4176.467674369398, -3374.998833557495, 'spam'),\n",
       " '71.words': (-269.42767181832943, -263.0576632707908, 'spam'),\n",
       " '72.words': (-2062.185028836458, -1751.6501929727435, 'spam'),\n",
       " '73.words': (-302.3936413550273, -306.2478193688259, 'ham'),\n",
       " '74.words': (-4151.442469001595, -3467.533192776957, 'spam'),\n",
       " '75.words': (-1877.271273523957, -1561.2303414336698, 'spam'),\n",
       " '76.words': (-2051.7663888439597, -1796.3326087808778, 'spam'),\n",
       " '77.words': (-1021.7978419506514, -1107.566323866011, 'ham'),\n",
       " '78.words': (-1117.4186584149131, -1169.6047018671122, 'ham'),\n",
       " '79.words': (-919.185782309535, -1061.20310775039, 'ham'),\n",
       " '8.words': (-5726.488105753498, -6449.359572400915, 'ham'),\n",
       " '80.words': (-634.5240382615665, -719.1697339064239, 'ham'),\n",
       " '81.words': (-2240.5711856013527, -1800.735992176026, 'spam'),\n",
       " '82.words': (-2904.6000273099316, -2478.740062406279, 'spam'),\n",
       " '83.words': (-2519.6073450417134, -2309.9921722676677, 'spam'),\n",
       " '84.words': (-199.11973058725843, -170.81998291880197, 'spam'),\n",
       " '85.words': (-1177.8728998083948, -1169.5729274957287, 'spam'),\n",
       " '86.words': (-1044.3785990461786, -1134.352830366582, 'ham'),\n",
       " '87.words': (-327.63070558288564, -325.0674062551652, 'spam'),\n",
       " '88.words': (-4070.683878779364, -4039.115049966269, 'spam'),\n",
       " '89.words': (-1764.3209663543603, -1910.0790716477932, 'ham'),\n",
       " '9.words': (-1293.7118162417667, -1445.1756433946596, 'ham'),\n",
       " '90.words': (-2419.2775065643623, -2641.8703441646917, 'ham'),\n",
       " '91.words': (-1072.137769680687, -1153.9640465917496, 'ham'),\n",
       " '92.words': (-1810.657502595442, -1821.8990237148662, 'ham'),\n",
       " '93.words': (-1630.972215133405, -1827.8850067327653, 'ham'),\n",
       " '94.words': (-265.85711666023724, -298.6139959836368, 'ham'),\n",
       " '95.words': (-1102.3818619964843, -1223.0984262210663, 'ham'),\n",
       " '96.words': (-1891.1274726170213, -2134.339357266899, 'ham'),\n",
       " '97.words': (-3350.0524639066643, -3486.7474066739846, 'ham'),\n",
       " '98.words': (-4705.18954016034, -6859.833973160583, 'ham'),\n",
       " '99.words': (-3721.330660281538, -3916.301316488432, 'ham')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HS_clf.predict(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.words': (-13889.803866366132, -12112.818810123108, 'spam'),\n",
       " '10.words': (-2806.472228896054, -2110.3206908330685, 'spam'),\n",
       " '100.words': (-142.8497197789069, -144.89242416522552, 'ham'),\n",
       " '11.words': (-1592.0246317527174, -1816.3439224762537, 'ham'),\n",
       " '12.words': (-939.2535584999176, -1067.7662681193776, 'ham'),\n",
       " '13.words': (-1873.2061964393417, -1500.4177414335259, 'spam'),\n",
       " '14.words': (-853.7749157228097, -771.6825300411613, 'spam'),\n",
       " '15.words': (-98.48199926759459, -79.12684755573657, 'spam'),\n",
       " '16.words': (-6520.260319432509, -6677.7621600451575, 'ham'),\n",
       " '17.words': (-358.85465514657017, -330.68649746241846, 'spam'),\n",
       " '18.words': (-1026.2760689863478, -798.4221803715476, 'spam'),\n",
       " '19.words': (-3738.600427964295, -3098.7829811053357, 'spam'),\n",
       " '2.words': (-801.2186044767166, -873.3344744827514, 'ham'),\n",
       " '20.words': (-2378.865791659679, -1806.0637582703584, 'spam'),\n",
       " '21.words': (-723.0139175060073, -623.98181291675, 'spam'),\n",
       " '22.words': (-540.0117286578485, -702.7549558478752, 'ham'),\n",
       " '23.words': (-1264.348987652736, -1188.3644096221317, 'spam'),\n",
       " '24.words': (-2258.5343158361884, -2222.086182435184, 'spam'),\n",
       " '25.words': (-531.5469156530337, -527.7951652806695, 'spam'),\n",
       " '26.words': (-1149.9258407348561, -1248.4389737064723, 'ham'),\n",
       " '27.words': (-1220.8007420283168, -1475.489846834085, 'ham'),\n",
       " '28.words': (-4046.9077427540756, -3014.9253494583854, 'spam'),\n",
       " '29.words': (-548.5925441751555, -648.3363043554474, 'ham'),\n",
       " '3.words': (-419.2241565619984, -496.8941252165459, 'ham'),\n",
       " '30.words': (-1545.337305835329, -1763.6182824429752, 'ham'),\n",
       " '31.words': (-1201.041717314829, -1014.5432497152785, 'spam'),\n",
       " '32.words': (-1018.489565643143, -983.125591485167, 'spam'),\n",
       " '33.words': (-1005.4081868747215, -1127.0864104605068, 'ham'),\n",
       " '34.words': (-2702.2037020326857, -2858.6492253213537, 'ham'),\n",
       " '35.words': (-759.3545263298779, -625.9895242147892, 'spam'),\n",
       " '36.words': (-2217.691983531547, -1564.0224864100494, 'spam'),\n",
       " '37.words': (-196.27085135462923, -150.63687644622323, 'spam'),\n",
       " '38.words': (-222.44600025931032, -220.93754510177874, 'spam'),\n",
       " '39.words': (-410.44805857526904, -499.13079238077444, 'ham'),\n",
       " '4.words': (-638.4571700378513, -686.3076666779336, 'ham'),\n",
       " '40.words': (-2163.765742486838, -2038.485849801313, 'spam'),\n",
       " '41.words': (-676.2452062498006, -720.673060651905, 'ham'),\n",
       " '42.words': (-957.5060593548994, -806.5812772386988, 'spam'),\n",
       " '43.words': (-235.63367538244555, -182.06806342796077, 'spam'),\n",
       " '44.words': (-159.68113532238928, -128.8494134966944, 'spam'),\n",
       " '45.words': (-3132.698246665337, -2323.2740122020664, 'spam'),\n",
       " '46.words': (-2463.036800391671, -1940.8973371440554, 'spam'),\n",
       " '47.words': (-526.1376392480189, -645.04254695912, 'ham'),\n",
       " '48.words': (-3698.0649026427977, -3970.9003514976453, 'ham'),\n",
       " '49.words': (-7737.089658539512, -8029.020270403326, 'ham'),\n",
       " '5.words': (-168.3311793297622, -167.9506320572737, 'spam'),\n",
       " '50.words': (-2806.472228896054, -2110.3206908330685, 'spam'),\n",
       " '51.words': (-228.81628942839748, -227.67332050932342, 'spam'),\n",
       " '52.words': (-567.594170902207, -675.0688116030846, 'ham'),\n",
       " '53.words': (-983.0033384275562, -1216.2893339836385, 'ham'),\n",
       " '54.words': (-842.2057160397339, -952.5000077058573, 'ham'),\n",
       " '55.words': (-9591.471489404697, -6924.240483084579, 'spam'),\n",
       " '56.words': (-2157.0578553425316, -2330.796131969487, 'ham'),\n",
       " '57.words': (-477.46220189456324, -550.1050206373088, 'ham'),\n",
       " '58.words': (-605.2035008360173, -570.683812143821, 'spam'),\n",
       " '59.words': (-2136.191600669753, -2482.8343097215215, 'ham'),\n",
       " '6.words': (-879.566083351657, -1023.428526560212, 'ham'),\n",
       " '60.words': (-1239.9039255550533, -1517.1877649438766, 'ham'),\n",
       " '61.words': (-983.7414248268053, -1134.824074462045, 'ham'),\n",
       " '62.words': (-715.3055116487113, -821.3436543384945, 'ham'),\n",
       " '63.words': (-923.3909943256068, -781.0483295006179, 'spam'),\n",
       " '64.words': (-1529.4010659568326, -1757.963358125464, 'ham'),\n",
       " '65.words': (-991.6797453600384, -769.8983589577115, 'spam'),\n",
       " '66.words': (-338.1821343976542, -336.6687590702737, 'spam'),\n",
       " '67.words': (-334.88083358966, -396.8827623338579, 'ham'),\n",
       " '68.words': (-723.0139175060073, -623.98181291675, 'spam'),\n",
       " '69.words': (-4035.894140085886, -3009.951033747854, 'spam'),\n",
       " '7.words': (-1196.7010066680289, -1524.4248151242384, 'ham'),\n",
       " '70.words': (-4032.7074578160523, -2913.7102470704167, 'spam'),\n",
       " '71.words': (-278.2001527246312, -255.7271258084781, 'spam'),\n",
       " '72.words': (-2068.178054686353, -1563.6131462203782, 'spam'),\n",
       " '73.words': (-301.2515386407736, -305.44508037271333, 'ham'),\n",
       " '74.words': (-4035.894140085886, -3015.108004865178, 'spam'),\n",
       " '75.words': (-1808.8621222118313, -1356.6085079791383, 'spam'),\n",
       " '76.words': (-1967.1366507937107, -1578.6355222677876, 'spam'),\n",
       " '77.words': (-961.673193486419, -1090.5424846839653, 'ham'),\n",
       " '78.words': (-1033.9782412226762, -1128.272454769669, 'ham'),\n",
       " '79.words': (-807.0504478826498, -1020.9067193713281, 'ham'),\n",
       " '8.words': (-5164.532050298132, -6522.036719711328, 'ham'),\n",
       " '80.words': (-572.4541291849114, -710.1181935408981, 'ham'),\n",
       " '81.words': (-2217.691983531547, -1564.0224864100494, 'spam'),\n",
       " '82.words': (-2827.965320682529, -2171.433507547752, 'spam'),\n",
       " '83.words': (-2386.8796933508856, -2110.212256963704, 'spam'),\n",
       " '84.words': (-211.2172240138139, -157.12520926255016, 'spam'),\n",
       " '85.words': (-1076.1616257966696, -1049.0881414820285, 'spam'),\n",
       " '86.words': (-986.6028184889757, -1128.1038147799823, 'ham'),\n",
       " '87.words': (-337.68870099271186, -333.5982892019883, 'spam'),\n",
       " '88.words': (-3823.3974238342676, -3764.325526515254, 'spam'),\n",
       " '89.words': (-1593.1019786826507, -1851.6000190373534, 'ham'),\n",
       " '9.words': (-1150.0562269876868, -1374.7428473887737, 'ham'),\n",
       " '90.words': (-2202.322730985177, -2526.3567879476045, 'ham'),\n",
       " '91.words': (-982.2337178752662, -1103.4388297733935, 'ham'),\n",
       " '92.words': (-1823.3647853338018, -1852.6831615996539, 'ham'),\n",
       " '93.words': (-1459.3100520217688, -1741.1064268621908, 'ham'),\n",
       " '94.words': (-238.7011481784268, -281.54248916561676, 'ham'),\n",
       " '95.words': (-1005.5336005923492, -1174.860490503553, 'ham'),\n",
       " '96.words': (-1729.0587254446218, -2084.358198457604, 'ham'),\n",
       " '97.words': (-3256.3426993358844, -3509.4602358247803, 'ham'),\n",
       " '98.words': (-4318.188485975773, -7591.506324068137, 'ham'),\n",
       " '99.words': (-3525.041615135468, -3837.1228042640196, 'ham')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HS_clf.tune_Laplace_params(.085)\n",
    "temp = HS_clf.predict(test_path)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam:  [1, 5, 10, 13, 14, 15, 17, 18, 19, 20, 21, 23, 24, 25, 28, 31, 32, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 50, 51, 55, 58, 63, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 81, 82, 83, 84, 85, 87, 88]\n",
      "\n",
      "ham:  [2, 3, 4, 6, 7, 8, 9, 11, 12, 16, 22, 26, 27, 29, 30, 33, 34, 39, 41, 47, 48, 49, 52, 53, 54, 56, 57, 59, 60, 61, 62, 64, 67, 73, 77, 78, 79, 80, 86, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "ham_pred = [] \n",
    "spam_pred = [] \n",
    "for key in temp: \n",
    "    if temp[key][2] == 'ham':\n",
    "        ham_pred.append(int(key.split('.')[0]))\n",
    "    else: \n",
    "        spam_pred.append(int(key.split('.')[0]))\n",
    "\n",
    "spam_pred.sort() \n",
    "ham_pred.sort()\n",
    "\n",
    "print('spam: ', spam_pred)\n",
    "print()\n",
    "print('ham: ', ham_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num spam predictions:  49\n",
      "num actual spam labels:  37\n"
     ]
    }
   ],
   "source": [
    "actual_spam = [1,5,10,13,14,15,17,18,19,20,21,23,\n",
    "          24,25,28,31,32,35,36,37,38,40,42,\n",
    "          43,44,45,46,50,51,55,58,63,65,66,\n",
    "          68,73,88]\n",
    "\n",
    "actual_ham = [val for val in (ham_pred + spam_pred) if val not in actual_spam]\n",
    "\n",
    "print('num spam predictions: ', len(spam_pred))\n",
    "print('num actual spam labels: ', len(actual_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP: things correctly classified as spam :  \n",
    "Tp = sum([i for i in spam_pred if i in actual_spam])\n",
    "\n",
    "# TN: Classified as ham, but it is ham\n",
    "Tn = sum([i for i in ham_pred if i in actual_ham]) \n",
    "\n",
    "# FP Classified as spam, but it is ham\n",
    "Fp = sum([i for i in spam_pred if i in actual_ham]) \n",
    "\n",
    "# FN: Classified as ham, but it is spam  \n",
    "Fn = sum([i for i in ham_pred if i in actual_spam]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.5603485838779957\n",
      "\n",
      "recall:  0.9462840323767476\n",
      "\n",
      "accuracy:  0.7857425742574258\n",
      "\n",
      "f1-score:  0.7038861521620142\n"
     ]
    }
   ],
   "source": [
    "# Precision: tp / (tp + fp) \n",
    "precision = Tp / (Tp + Fp)\n",
    "\n",
    "# Recall: tp / (tp+fn) # what proportion of the world is spam \n",
    "recall = Tp / (Tp + Fn) \n",
    "\n",
    "# Accuracy: (tp + tn) / (tp + tn + fp + fn)\n",
    "acc = (Tp + Tn) / (Tp + Tn + Fp + Fn)\n",
    "\n",
    "f1 = 2 * (precision*recall) / (precision + recall) \n",
    "\n",
    "print('precision: ', precision) \n",
    "print()\n",
    "print('recall: ', recall) \n",
    "print() \n",
    "print('accuracy: ', acc)\n",
    "print()\n",
    "print('f1-score: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://classeval.wordpress.com/introduction/basic-evaluation-measures/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive bayes:**  \n",
    "    Naive bayes classification is a means of finding the most likely class label give a set of words. In this case we worked\n",
    "    with a binary classification model, where the classes were ham and spam, and we pre-generated tables based on co-occurunces\n",
    "    of words from a training set. the pre-computed values could then be used to classify test instances. Overall, Naive Bayes\n",
    "    can be simply thought of as a way of pre-counting co-occurances of words to map new patterns in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Code:**   \n",
    "I generated the HamSpamClassifier to predict whether an email is ham or spam. The user starts by building a model from the Ham and Spam directories. This is done using the .fit() method. the fit() will generate a table for ham and spam based on the logProb of a given word. Once the fit is completed, the user uses the .predict() function to pass in his/her test set. The user can also call the tune method, which updates all the ham and spam logprobs based on a new alpha. Later I will implement an evaluation method to compute the f1_score of the model. The above cells, where I compute the precision, recall, accuracy, would be built into the classifier so a user could evaluate their outcome directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
